{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import sys\n",
    "sys.path.append(\"E:\\PythonModels\\segment-anything\")\n",
    "sys.path.append(\"E:\\PythonModels\\segment-anything\\modeling\")\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
    "from segment_anything.modeling import ImageEncoderViT\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = ImageEncoderViT(img_size=256, out_chans=32).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      | 335453 KiB | 335453 KiB | 335453 KiB |      0 B   |\\n|       from large pool | 334080 KiB | 334080 KiB | 334080 KiB |      0 B   |\\n|       from small pool |   1373 KiB |   1373 KiB |   1373 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         | 335453 KiB | 335453 KiB | 335453 KiB |      0 B   |\\n|       from large pool | 334080 KiB | 334080 KiB | 334080 KiB |      0 B   |\\n|       from small pool |   1373 KiB |   1373 KiB |   1373 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      | 335451 KiB | 335451 KiB | 335451 KiB |      0 B   |\\n|       from large pool | 334080 KiB | 334080 KiB | 334080 KiB |      0 B   |\\n|       from small pool |   1371 KiB |   1371 KiB |   1371 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |\\n|       from large pool | 389120 KiB | 389120 KiB | 389120 KiB |      0 B   |\\n|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  55715 KiB |  56620 KiB | 264445 KiB | 208730 KiB |\\n|       from large pool |  55040 KiB |  55040 KiB | 262400 KiB | 207360 KiB |\\n|       from small pool |    675 KiB |   2045 KiB |   2045 KiB |   1370 KiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     153    |     153    |     153    |       0    |\\n|       from large pool |      49    |      49    |      49    |       0    |\\n|       from small pool |     104    |     104    |     104    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     153    |     153    |     153    |       0    |\\n|       from large pool |      49    |      49    |      49    |       0    |\\n|       from small pool |     104    |     104    |     104    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      20    |      20    |      20    |       0    |\\n|       from large pool |      19    |      19    |      19    |       0    |\\n|       from small pool |       1    |       1    |       1    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      20    |      20    |      20    |       0    |\\n|       from large pool |      19    |      19    |      19    |       0    |\\n|       from small pool |       1    |       1    |       1    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 16, 16]         590,592\n",
      "        PatchEmbed-2          [-1, 16, 16, 768]               0\n",
      "         LayerNorm-3          [-1, 16, 16, 768]           1,536\n",
      "            Linear-4         [-1, 16, 16, 2304]       1,771,776\n",
      "            Linear-5          [-1, 16, 16, 768]         590,592\n",
      "         Attention-6          [-1, 16, 16, 768]               0\n",
      "         LayerNorm-7          [-1, 16, 16, 768]           1,536\n",
      "            Linear-8         [-1, 16, 16, 3072]       2,362,368\n",
      "              GELU-9         [-1, 16, 16, 3072]               0\n",
      "           Linear-10          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-11          [-1, 16, 16, 768]               0\n",
      "            Block-12          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-13          [-1, 16, 16, 768]           1,536\n",
      "           Linear-14         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-15          [-1, 16, 16, 768]         590,592\n",
      "        Attention-16          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-17          [-1, 16, 16, 768]           1,536\n",
      "           Linear-18         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-19         [-1, 16, 16, 3072]               0\n",
      "           Linear-20          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-21          [-1, 16, 16, 768]               0\n",
      "            Block-22          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-23          [-1, 16, 16, 768]           1,536\n",
      "           Linear-24         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-25          [-1, 16, 16, 768]         590,592\n",
      "        Attention-26          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-27          [-1, 16, 16, 768]           1,536\n",
      "           Linear-28         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-29         [-1, 16, 16, 3072]               0\n",
      "           Linear-30          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-31          [-1, 16, 16, 768]               0\n",
      "            Block-32          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-33          [-1, 16, 16, 768]           1,536\n",
      "           Linear-34         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-35          [-1, 16, 16, 768]         590,592\n",
      "        Attention-36          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-37          [-1, 16, 16, 768]           1,536\n",
      "           Linear-38         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-39         [-1, 16, 16, 3072]               0\n",
      "           Linear-40          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-41          [-1, 16, 16, 768]               0\n",
      "            Block-42          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-43          [-1, 16, 16, 768]           1,536\n",
      "           Linear-44         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-45          [-1, 16, 16, 768]         590,592\n",
      "        Attention-46          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-47          [-1, 16, 16, 768]           1,536\n",
      "           Linear-48         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-49         [-1, 16, 16, 3072]               0\n",
      "           Linear-50          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-51          [-1, 16, 16, 768]               0\n",
      "            Block-52          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-53          [-1, 16, 16, 768]           1,536\n",
      "           Linear-54         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-55          [-1, 16, 16, 768]         590,592\n",
      "        Attention-56          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-57          [-1, 16, 16, 768]           1,536\n",
      "           Linear-58         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-59         [-1, 16, 16, 3072]               0\n",
      "           Linear-60          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-61          [-1, 16, 16, 768]               0\n",
      "            Block-62          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-63          [-1, 16, 16, 768]           1,536\n",
      "           Linear-64         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-65          [-1, 16, 16, 768]         590,592\n",
      "        Attention-66          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-67          [-1, 16, 16, 768]           1,536\n",
      "           Linear-68         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-69         [-1, 16, 16, 3072]               0\n",
      "           Linear-70          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-71          [-1, 16, 16, 768]               0\n",
      "            Block-72          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-73          [-1, 16, 16, 768]           1,536\n",
      "           Linear-74         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-75          [-1, 16, 16, 768]         590,592\n",
      "        Attention-76          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-77          [-1, 16, 16, 768]           1,536\n",
      "           Linear-78         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-79         [-1, 16, 16, 3072]               0\n",
      "           Linear-80          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-81          [-1, 16, 16, 768]               0\n",
      "            Block-82          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-83          [-1, 16, 16, 768]           1,536\n",
      "           Linear-84         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-85          [-1, 16, 16, 768]         590,592\n",
      "        Attention-86          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-87          [-1, 16, 16, 768]           1,536\n",
      "           Linear-88         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-89         [-1, 16, 16, 3072]               0\n",
      "           Linear-90          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-91          [-1, 16, 16, 768]               0\n",
      "            Block-92          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-93          [-1, 16, 16, 768]           1,536\n",
      "           Linear-94         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-95          [-1, 16, 16, 768]         590,592\n",
      "        Attention-96          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-97          [-1, 16, 16, 768]           1,536\n",
      "           Linear-98         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-99         [-1, 16, 16, 3072]               0\n",
      "          Linear-100          [-1, 16, 16, 768]       2,360,064\n",
      "        MLPBlock-101          [-1, 16, 16, 768]               0\n",
      "           Block-102          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-103          [-1, 16, 16, 768]           1,536\n",
      "          Linear-104         [-1, 16, 16, 2304]       1,771,776\n",
      "          Linear-105          [-1, 16, 16, 768]         590,592\n",
      "       Attention-106          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-107          [-1, 16, 16, 768]           1,536\n",
      "          Linear-108         [-1, 16, 16, 3072]       2,362,368\n",
      "            GELU-109         [-1, 16, 16, 3072]               0\n",
      "          Linear-110          [-1, 16, 16, 768]       2,360,064\n",
      "        MLPBlock-111          [-1, 16, 16, 768]               0\n",
      "           Block-112          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-113          [-1, 16, 16, 768]           1,536\n",
      "          Linear-114         [-1, 16, 16, 2304]       1,771,776\n",
      "          Linear-115          [-1, 16, 16, 768]         590,592\n",
      "       Attention-116          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-117          [-1, 16, 16, 768]           1,536\n",
      "          Linear-118         [-1, 16, 16, 3072]       2,362,368\n",
      "            GELU-119         [-1, 16, 16, 3072]               0\n",
      "          Linear-120          [-1, 16, 16, 768]       2,360,064\n",
      "        MLPBlock-121          [-1, 16, 16, 768]               0\n",
      "           Block-122          [-1, 16, 16, 768]               0\n",
      "          Conv2d-123           [-1, 32, 16, 16]          24,576\n",
      "     LayerNorm2d-124           [-1, 32, 16, 16]              64\n",
      "          Conv2d-125           [-1, 32, 16, 16]           9,216\n",
      "     LayerNorm2d-126           [-1, 32, 16, 16]              64\n",
      "================================================================\n",
      "Total params: 85,678,976\n",
      "Trainable params: 85,678,976\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 327.25\n",
      "Params size (MB): 326.84\n",
      "Estimated Total Size (MB): 654.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=( 3, 256,256))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "random_rgb_tensor = torch.randn(1, 3, 256, 256).to(device)\n",
    "\n",
    "prediction = model(random_rgb_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],\n       device='cuda:0', grad_fn=<AddBackward0>)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 16, 16]         590,592\n",
      "        PatchEmbed-2          [-1, 16, 16, 768]               0\n",
      "         LayerNorm-3          [-1, 16, 16, 768]           1,536\n",
      "            Linear-4         [-1, 16, 16, 2304]       1,771,776\n",
      "            Linear-5          [-1, 16, 16, 768]         590,592\n",
      "         Attention-6          [-1, 16, 16, 768]               0\n",
      "         LayerNorm-7          [-1, 16, 16, 768]           1,536\n",
      "            Linear-8         [-1, 16, 16, 3072]       2,362,368\n",
      "              GELU-9         [-1, 16, 16, 3072]               0\n",
      "           Linear-10          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-11          [-1, 16, 16, 768]               0\n",
      "            Block-12          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-13          [-1, 16, 16, 768]           1,536\n",
      "           Linear-14         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-15          [-1, 16, 16, 768]         590,592\n",
      "        Attention-16          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-17          [-1, 16, 16, 768]           1,536\n",
      "           Linear-18         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-19         [-1, 16, 16, 3072]               0\n",
      "           Linear-20          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-21          [-1, 16, 16, 768]               0\n",
      "            Block-22          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-23          [-1, 16, 16, 768]           1,536\n",
      "           Linear-24         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-25          [-1, 16, 16, 768]         590,592\n",
      "        Attention-26          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-27          [-1, 16, 16, 768]           1,536\n",
      "           Linear-28         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-29         [-1, 16, 16, 3072]               0\n",
      "           Linear-30          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-31          [-1, 16, 16, 768]               0\n",
      "            Block-32          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-33          [-1, 16, 16, 768]           1,536\n",
      "           Linear-34         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-35          [-1, 16, 16, 768]         590,592\n",
      "        Attention-36          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-37          [-1, 16, 16, 768]           1,536\n",
      "           Linear-38         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-39         [-1, 16, 16, 3072]               0\n",
      "           Linear-40          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-41          [-1, 16, 16, 768]               0\n",
      "            Block-42          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-43          [-1, 16, 16, 768]           1,536\n",
      "           Linear-44         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-45          [-1, 16, 16, 768]         590,592\n",
      "        Attention-46          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-47          [-1, 16, 16, 768]           1,536\n",
      "           Linear-48         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-49         [-1, 16, 16, 3072]               0\n",
      "           Linear-50          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-51          [-1, 16, 16, 768]               0\n",
      "            Block-52          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-53          [-1, 16, 16, 768]           1,536\n",
      "           Linear-54         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-55          [-1, 16, 16, 768]         590,592\n",
      "        Attention-56          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-57          [-1, 16, 16, 768]           1,536\n",
      "           Linear-58         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-59         [-1, 16, 16, 3072]               0\n",
      "           Linear-60          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-61          [-1, 16, 16, 768]               0\n",
      "            Block-62          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-63          [-1, 16, 16, 768]           1,536\n",
      "           Linear-64         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-65          [-1, 16, 16, 768]         590,592\n",
      "        Attention-66          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-67          [-1, 16, 16, 768]           1,536\n",
      "           Linear-68         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-69         [-1, 16, 16, 3072]               0\n",
      "           Linear-70          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-71          [-1, 16, 16, 768]               0\n",
      "            Block-72          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-73          [-1, 16, 16, 768]           1,536\n",
      "           Linear-74         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-75          [-1, 16, 16, 768]         590,592\n",
      "        Attention-76          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-77          [-1, 16, 16, 768]           1,536\n",
      "           Linear-78         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-79         [-1, 16, 16, 3072]               0\n",
      "           Linear-80          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-81          [-1, 16, 16, 768]               0\n",
      "            Block-82          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-83          [-1, 16, 16, 768]           1,536\n",
      "           Linear-84         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-85          [-1, 16, 16, 768]         590,592\n",
      "        Attention-86          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-87          [-1, 16, 16, 768]           1,536\n",
      "           Linear-88         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-89         [-1, 16, 16, 3072]               0\n",
      "           Linear-90          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-91          [-1, 16, 16, 768]               0\n",
      "            Block-92          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-93          [-1, 16, 16, 768]           1,536\n",
      "           Linear-94         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-95          [-1, 16, 16, 768]         590,592\n",
      "        Attention-96          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-97          [-1, 16, 16, 768]           1,536\n",
      "           Linear-98         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-99         [-1, 16, 16, 3072]               0\n",
      "          Linear-100          [-1, 16, 16, 768]       2,360,064\n",
      "        MLPBlock-101          [-1, 16, 16, 768]               0\n",
      "           Block-102          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-103          [-1, 16, 16, 768]           1,536\n",
      "          Linear-104         [-1, 16, 16, 2304]       1,771,776\n",
      "          Linear-105          [-1, 16, 16, 768]         590,592\n",
      "       Attention-106          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-107          [-1, 16, 16, 768]           1,536\n",
      "          Linear-108         [-1, 16, 16, 3072]       2,362,368\n",
      "            GELU-109         [-1, 16, 16, 3072]               0\n",
      "          Linear-110          [-1, 16, 16, 768]       2,360,064\n",
      "        MLPBlock-111          [-1, 16, 16, 768]               0\n",
      "           Block-112          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-113          [-1, 16, 16, 768]           1,536\n",
      "          Linear-114         [-1, 16, 16, 2304]       1,771,776\n",
      "          Linear-115          [-1, 16, 16, 768]         590,592\n",
      "       Attention-116          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-117          [-1, 16, 16, 768]           1,536\n",
      "          Linear-118         [-1, 16, 16, 3072]       2,362,368\n",
      "            GELU-119         [-1, 16, 16, 3072]               0\n",
      "          Linear-120          [-1, 16, 16, 768]       2,360,064\n",
      "        MLPBlock-121          [-1, 16, 16, 768]               0\n",
      "           Block-122          [-1, 16, 16, 768]               0\n",
      "          Conv2d-123           [-1, 16, 16, 16]          12,288\n",
      "     LayerNorm2d-124           [-1, 16, 16, 16]              32\n",
      "          Conv2d-125           [-1, 16, 16, 16]           2,304\n",
      "     LayerNorm2d-126           [-1, 16, 16, 16]              32\n",
      " ImageEncoderViT-127           [-1, 16, 16, 16]               0\n",
      " ConvTranspose2d-128           [-1, 16, 32, 32]           1,040\n",
      " ConvTranspose2d-129           [-1, 16, 64, 64]           1,040\n",
      " ConvTranspose2d-130         [-1, 16, 128, 128]           1,040\n",
      " ConvTranspose2d-131        [-1, 104, 256, 256]           6,760\n",
      "================================================================\n",
      "Total params: 85,669,592\n",
      "Trainable params: 85,669,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 381.78\n",
      "Params size (MB): 326.80\n",
      "Estimated Total Size (MB): 709.33\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from model.architectures.SAM_Architecture import SAM_Architecture\n",
    "\n",
    "model2 = SAM_Architecture(104).to(device)\n",
    "summary(model2, input_size=( 3, 256,256))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

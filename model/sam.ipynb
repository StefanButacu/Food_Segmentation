{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import sys\n",
    "sys.path.append(\"E:\\PythonModels\\segment-anything\")\n",
    "sys.path.append(\"E:\\PythonModels\\segment-anything\\modeling\")\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
    "from segment_anything.modeling import ImageEncoderViT\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = ImageEncoderViT(img_size=256, out_chans=32).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 16, 16]         590,592\n",
      "        PatchEmbed-2          [-1, 16, 16, 768]               0\n",
      "         LayerNorm-3          [-1, 16, 16, 768]           1,536\n",
      "            Linear-4         [-1, 16, 16, 2304]       1,771,776\n",
      "            Linear-5          [-1, 16, 16, 768]         590,592\n",
      "         Attention-6          [-1, 16, 16, 768]               0\n",
      "         LayerNorm-7          [-1, 16, 16, 768]           1,536\n",
      "            Linear-8         [-1, 16, 16, 3072]       2,362,368\n",
      "              GELU-9         [-1, 16, 16, 3072]               0\n",
      "           Linear-10          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-11          [-1, 16, 16, 768]               0\n",
      "            Block-12          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-13          [-1, 16, 16, 768]           1,536\n",
      "           Linear-14         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-15          [-1, 16, 16, 768]         590,592\n",
      "        Attention-16          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-17          [-1, 16, 16, 768]           1,536\n",
      "           Linear-18         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-19         [-1, 16, 16, 3072]               0\n",
      "           Linear-20          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-21          [-1, 16, 16, 768]               0\n",
      "            Block-22          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-23          [-1, 16, 16, 768]           1,536\n",
      "           Linear-24         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-25          [-1, 16, 16, 768]         590,592\n",
      "        Attention-26          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-27          [-1, 16, 16, 768]           1,536\n",
      "           Linear-28         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-29         [-1, 16, 16, 3072]               0\n",
      "           Linear-30          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-31          [-1, 16, 16, 768]               0\n",
      "            Block-32          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-33          [-1, 16, 16, 768]           1,536\n",
      "           Linear-34         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-35          [-1, 16, 16, 768]         590,592\n",
      "        Attention-36          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-37          [-1, 16, 16, 768]           1,536\n",
      "           Linear-38         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-39         [-1, 16, 16, 3072]               0\n",
      "           Linear-40          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-41          [-1, 16, 16, 768]               0\n",
      "            Block-42          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-43          [-1, 16, 16, 768]           1,536\n",
      "           Linear-44         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-45          [-1, 16, 16, 768]         590,592\n",
      "        Attention-46          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-47          [-1, 16, 16, 768]           1,536\n",
      "           Linear-48         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-49         [-1, 16, 16, 3072]               0\n",
      "           Linear-50          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-51          [-1, 16, 16, 768]               0\n",
      "            Block-52          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-53          [-1, 16, 16, 768]           1,536\n",
      "           Linear-54         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-55          [-1, 16, 16, 768]         590,592\n",
      "        Attention-56          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-57          [-1, 16, 16, 768]           1,536\n",
      "           Linear-58         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-59         [-1, 16, 16, 3072]               0\n",
      "           Linear-60          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-61          [-1, 16, 16, 768]               0\n",
      "            Block-62          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-63          [-1, 16, 16, 768]           1,536\n",
      "           Linear-64         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-65          [-1, 16, 16, 768]         590,592\n",
      "        Attention-66          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-67          [-1, 16, 16, 768]           1,536\n",
      "           Linear-68         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-69         [-1, 16, 16, 3072]               0\n",
      "           Linear-70          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-71          [-1, 16, 16, 768]               0\n",
      "            Block-72          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-73          [-1, 16, 16, 768]           1,536\n",
      "           Linear-74         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-75          [-1, 16, 16, 768]         590,592\n",
      "        Attention-76          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-77          [-1, 16, 16, 768]           1,536\n",
      "           Linear-78         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-79         [-1, 16, 16, 3072]               0\n",
      "           Linear-80          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-81          [-1, 16, 16, 768]               0\n",
      "            Block-82          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-83          [-1, 16, 16, 768]           1,536\n",
      "           Linear-84         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-85          [-1, 16, 16, 768]         590,592\n",
      "        Attention-86          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-87          [-1, 16, 16, 768]           1,536\n",
      "           Linear-88         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-89         [-1, 16, 16, 3072]               0\n",
      "           Linear-90          [-1, 16, 16, 768]       2,360,064\n",
      "         MLPBlock-91          [-1, 16, 16, 768]               0\n",
      "            Block-92          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-93          [-1, 16, 16, 768]           1,536\n",
      "           Linear-94         [-1, 16, 16, 2304]       1,771,776\n",
      "           Linear-95          [-1, 16, 16, 768]         590,592\n",
      "        Attention-96          [-1, 16, 16, 768]               0\n",
      "        LayerNorm-97          [-1, 16, 16, 768]           1,536\n",
      "           Linear-98         [-1, 16, 16, 3072]       2,362,368\n",
      "             GELU-99         [-1, 16, 16, 3072]               0\n",
      "          Linear-100          [-1, 16, 16, 768]       2,360,064\n",
      "        MLPBlock-101          [-1, 16, 16, 768]               0\n",
      "           Block-102          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-103          [-1, 16, 16, 768]           1,536\n",
      "          Linear-104         [-1, 16, 16, 2304]       1,771,776\n",
      "          Linear-105          [-1, 16, 16, 768]         590,592\n",
      "       Attention-106          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-107          [-1, 16, 16, 768]           1,536\n",
      "          Linear-108         [-1, 16, 16, 3072]       2,362,368\n",
      "            GELU-109         [-1, 16, 16, 3072]               0\n",
      "          Linear-110          [-1, 16, 16, 768]       2,360,064\n",
      "        MLPBlock-111          [-1, 16, 16, 768]               0\n",
      "           Block-112          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-113          [-1, 16, 16, 768]           1,536\n",
      "          Linear-114         [-1, 16, 16, 2304]       1,771,776\n",
      "          Linear-115          [-1, 16, 16, 768]         590,592\n",
      "       Attention-116          [-1, 16, 16, 768]               0\n",
      "       LayerNorm-117          [-1, 16, 16, 768]           1,536\n",
      "          Linear-118         [-1, 16, 16, 3072]       2,362,368\n",
      "            GELU-119         [-1, 16, 16, 3072]               0\n",
      "          Linear-120          [-1, 16, 16, 768]       2,360,064\n",
      "        MLPBlock-121          [-1, 16, 16, 768]               0\n",
      "           Block-122          [-1, 16, 16, 768]               0\n",
      "          Conv2d-123           [-1, 32, 16, 16]          24,576\n",
      "     LayerNorm2d-124           [-1, 32, 16, 16]              64\n",
      "          Conv2d-125           [-1, 32, 16, 16]           9,216\n",
      "     LayerNorm2d-126           [-1, 32, 16, 16]              64\n",
      "================================================================\n",
      "Total params: 85,678,976\n",
      "Trainable params: 85,678,976\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 327.25\n",
      "Params size (MB): 326.84\n",
      "Estimated Total Size (MB): 654.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=( 3, 256,256))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "random_rgb_tensor = torch.randn(1, 3, 256, 256).to(device)\n",
    "\n",
    "prediction = model(random_rgb_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from model.architectures.SAM_Architecture import SAM_Architecture\n",
    "\n",
    "model2 = SAM_Architecture(104).to(device)\n",
    "summary(model2, input_size=( 3, 256,256))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

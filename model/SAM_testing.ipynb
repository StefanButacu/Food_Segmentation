{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "\n",
    "from model.FoodDataset import FoodDataset\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "image_paths = [\"E:\\Licenta_DOC\\API_Segmentation\\data\\generated\\img_dir\"]\n",
    "seg_paths = [\"E:\\Licenta_DOC\\API_Segmentation\\data\\generated\\\\ann_dir\"]\n",
    "\n",
    "LOAD_MODEL = False\n",
    "def get_images(transform = None,batch_size=32,shuffle=True,pin_memory=True):\n",
    "    data = FoodDataset(image_paths, seg_paths,transform = transform)\n",
    "    train_size = int(0.8 * data.__len__())\n",
    "    test_size = data.__len__() - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "    train_batch = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n",
    "    test_batch = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n",
    "    return train_batch,test_batch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "from service.Service import Service\n",
    "\n",
    "t1 = A.Compose([\n",
    "    A.Resize(128, 128),\n",
    "    A.augmentations.transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "train_batch,test_batch = get_images(transform =t1, batch_size=8)\n",
    "\n",
    "service = Service()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for img,mask in train_batch:\n",
    "    img1 = np.transpose(img[0,:,:,:],(1,2,0))\n",
    "    mask1 = np.array(mask[0,:,:])\n",
    "    img2 = np.transpose(img[1,:,:,:],(1,2,0))\n",
    "    mask2 = np.array(mask[1,:,:])\n",
    "    img3 = np.transpose(img[2,:,:,:],(1,2,0))\n",
    "    mask3 = np.array(mask[2,:,:])\n",
    "    fig , ax =  plt.subplots(2, 2, figsize=(18, 18))\n",
    "    ax[0][0].imshow(img1)\n",
    "    ax[0][1].imshow(mask1)\n",
    "    print(service.get_bounding_boxes(mask1))\n",
    "    break\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"E:\\PythonModels\\segment-anything\")\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = np.array(Image.open('E:\\PythonModels\\segment-anything\\\\notebooks\\images\\\\truck.jpg'))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sam_checkpoint = \"checkpoints/sam_vit_h_4b8939.pth\"\n",
    "# model_type = \"vit_h\"\n",
    "\n",
    "sam_checkpoint = \"checkpoints/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "\n",
    "# predictor = SamPredictor(sam)\n",
    "print(sam.image_encoder)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "model = sam.image_encoder.to(device)\n",
    "print(summary(model, (3, 1024, 1024)))\n",
    "print(sam)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageEncoderViT(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (neck): Sequential(\n",
      "    (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): LayerNorm2d()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (3): LayerNorm2d()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layers = { name: module for name,module in sam.named_children()}\n",
    "image_encoder = layers.get('image_encoder').to('cuda')\n",
    "print(image_encoder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "sam_image_encoder = sam.image_encoder.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input = torch.randn(1, 3, 1024, 1024).to(device)\n",
    "\n",
    "x = sam_image_encoder(input)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# without_neck = { name: module for name,module in image_encoder.named_children() if name != 'neck'}\n",
    "# print(without_neck)\n",
    "# class CustomModel(torch.nn.Module) :\n",
    "#     def __init__(self, patch_embed, blocks):\n",
    "#         super(CustomModel, self).__init__()\n",
    "#         self.patch_embed = patch_embed\n",
    "#         self.blocks = blocks\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embed(x)\n",
    "#         x = self.blocks(x)\n",
    "#         return x\n",
    "#\n",
    "#\n",
    "# model = CustomModel(**without_neck)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "print(summary(sam.image_encoder, input_size=(3, 1024, 1024)))\n",
    "# image_encoder(torch.rand(1, 3 , 1024, 1024).to(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "predictor.set_image(image)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_point = np.array([[500, 375]])\n",
    "input_label = np.array([1])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from model.checkpoints.checkpoints import load_checkpoint\n",
    "from model.architectures.UnetRestNet50 import UNetResNet152\n",
    "service = Service()\n",
    "MODEL_PRETRAINED_FILE = 'checkpoints/checkpoint-pretrain.pth.tar'\n",
    "model = UNetResNet152(104).to(DEVICE)\n",
    "load_checkpoint(torch.load(MODEL_PRETRAINED_FILE), model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "image = Image.open('E:\\Licenta_DOC\\API_Segmentation\\data\\generated\\img_dir\\\\00000000_aug_0.jpg')\n",
    "x = np.array(image)\n",
    "image = image.resize((256,256))\n",
    "image = np.array(image)\n",
    "t1 = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.augmentations.transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "x = t1(image=x)['image']\n",
    "x = x.to(DEVICE)\n",
    "x = x.unsqueeze(0)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "preds = torch.argmax(softmax(model(x)), axis=1).to('cpu')\n",
    "preds1 = np.array(preds[0, :, :])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "boxes = service.get_bounding_boxes(preds1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.category_reader import read_categories\n",
    "\n",
    "CATEGORY_DICT_FILE = '../data/category_id.txt'\n",
    "category_dict = read_categories(CATEGORY_DICT_FILE)\n",
    "print(category_dict['48'])\n",
    "print(category_dict['61'])\n",
    "print(category_dict['47'])\n",
    "print(category_dict['80'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictor.set_image(np.array(image))\n",
    "\n",
    "input_boxes = torch.tensor([[t_x, t_y, b_x, b_y] for c, (t_x, t_y), (b_x, b_y) in boxes]\n",
    ", device=predictor.device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
    "masks, _, _ = predictor.predict_torch(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    boxes=transformed_boxes,\n",
    "    multimask_output=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(masks.shape)  # (batch_size) x (num_predicted_masks_per_input) x H x W\n",
    "print(masks[0])\n",
    "unique_elements = torch.unique(masks[0])\n",
    "print(unique_elements)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "for mask in masks:\n",
    "    show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)\n",
    "for box in input_boxes:\n",
    "    show_box(box.cpu().numpy(), plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mask Generator -> Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(model=sam,\n",
    "    points_per_side=16,\n",
    "    pred_iou_thresh=0.92,\n",
    "    stability_score_thresh=0.96,\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "   )\n",
    "masks = mask_generator.generate(image)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    polygons = []\n",
    "    color = []\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        img = np.ones((m.shape[0], m.shape[1], 3))\n",
    "        color_mask = np.random.random((1, 3)).tolist()[0]\n",
    "        for i in range(3):\n",
    "            img[:,:,i] = color_mask[i]\n",
    "        ax.imshow(np.dstack((img, m*0.35)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(masks[0].keys())\n",
    "masks = [ mask for mask in masks if mask['predicted_iou'] >= 0.95 ]\n",
    "print(len(masks))\n",
    "masks = sorted(masks, key=lambda x: x['area'])\n",
    "masks = masks[::-1]\n",
    "masks = masks[:8]\n",
    "for mask in masks:\n",
    "    print(\"Area=\", mask['area'], )\n",
    "    print(\"Stability_score=\", mask['stability_score'])\n",
    "    print(\"predicted_iou=\", mask['predicted_iou'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
